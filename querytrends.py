from trendspy import Trends
import pandas as pd
import json
import time
import random
from datetime import datetime
import requests
from urllib.parse import quote
import re

def get_related_queries(keyword, geo='', timeframe='today 12-m'):
    """
    è·å–å…³é”®è¯çš„ç›¸å…³æŸ¥è¯¢æ•°æ®ï¼Œå¸¦è¯·æ±‚é™åˆ¶
    """
    while True:  # æ·»åŠ æ— é™é‡è¯•å¾ªç¯
        tr = Trends(hl='zh-CN')
        
        # éšæœºåŒ– User-Agent
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        headers = {
            'referer': 'https://www.google.com/',
            'User-Agent': random.choice(user_agents),
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        }
        
        try:
            print(f"[{keyword}] å¼€å§‹æŸ¥è¯¢...", flush=True)

            # æ£€æŸ¥è¯·æ±‚é™åˆ¶
            print(f"[{keyword}] æ£€æŸ¥è¯·æ±‚é™åˆ¶...", flush=True)
            request_limiter.wait_if_needed()
            print(f"[{keyword}] è¯·æ±‚é™åˆ¶æ£€æŸ¥é€šè¿‡", flush=True)

            # æ·»åŠ éšæœºå»¶æ—¶
            delay = random.uniform(1, 3)
            print(f"[{keyword}] ç­‰å¾… {delay:.1f} ç§’åå¼€å§‹è¯·æ±‚...", flush=True)
            time.sleep(delay)

            print(f"[{keyword}] æ­£åœ¨è°ƒç”¨ Google Trends API (å¯èƒ½éœ€è¦30-120ç§’)...", flush=True)
            related_data = tr.related_queries(
                keyword,
                headers=headers,
                geo=geo,
                timeframe=timeframe
            )
            print(f"[{keyword}] æˆåŠŸè·å–æ•°æ®ï¼", flush=True)
            return related_data
            
        except Exception as e:
            error_msg = str(e)
            print(f"[{keyword}] âŒ å°è¯•è·å–æ•°æ®æ—¶å‡ºé”™: {error_msg}", flush=True)

            # å¦‚æœæ˜¯é…é¢è¶…é™é”™è¯¯ï¼Œç­‰å¾…åé‡è¯•
            if "API quota exceeded" in error_msg:
                wait_time = random.uniform(300, 360)  # ç­‰å¾…5-6åˆ†é’Ÿ
                print(f"[{keyword}] âš ï¸  APIé…é¢è¶…é™ï¼Œç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...", flush=True)
                time.sleep(wait_time)
                print(f"[{keyword}] ğŸ”„ å¼€å§‹é‡è¯•...", flush=True)
                continue  # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•

            # å¦‚æœæ˜¯NoneTypeé”™è¯¯ï¼Œä¹Ÿç­‰å¾…åé‡è¯•
            if "'NoneType' object has no attribute 'raise_for_status'" in error_msg:
                wait_time = random.uniform(60, 120)  # ç­‰å¾…1-2åˆ†é’Ÿ
                print(f"[{keyword}] âš ï¸  è¯·æ±‚è¿”å›ä¸ºç©ºï¼Œç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...", flush=True)
                time.sleep(wait_time)
                print(f"[{keyword}] ğŸ”„ å¼€å§‹é‡è¯•...", flush=True)
                continue  # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•

            # å…¶ä»–é”™è¯¯åˆ™ç›´æ¥æŠ›å‡º
            print(f"[{keyword}] âŒ é‡åˆ°ä¸å¯æ¢å¤çš„é”™è¯¯ï¼Œåœæ­¢é‡è¯•", flush=True)
            raise

def batch_get_queries(keywords, geo='', timeframe='today 12-m', delay_between_queries=5):
    """
    æ‰¹é‡è·å–å¤šä¸ªå…³é”®è¯çš„æ•°æ®ï¼Œå¸¦é—´éš”æ§åˆ¶
    """
    results = {}

    print(f"\n{'='*60}", flush=True)
    print(f"å¼€å§‹æ‰¹é‡æŸ¥è¯¢ {len(keywords)} ä¸ªå…³é”®è¯", flush=True)
    print(f"{'='*60}\n", flush=True)

    for idx, keyword in enumerate(keywords, 1):
        try:
            print(f"\n[{idx}/{len(keywords)}] å¼€å§‹å¤„ç†å…³é”®è¯: {keyword}", flush=True)
            results[keyword] = get_related_queries(keyword, geo, timeframe)

            # åœ¨è¯·æ±‚ä¹‹é—´æ·»åŠ å»¶æ—¶
            if keyword != keywords[-1]:  # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªå…³é”®è¯
                delay = delay_between_queries + random.uniform(0, 2)  # åŸºç¡€å»¶æ—¶åŠ 0-2ç§’çš„éšæœºå»¶æ—¶
                print(f"[{keyword}] âœ… å®Œæˆï¼Œç­‰å¾… {delay:.1f} ç§’åç»§ç»­ä¸‹ä¸€ä¸ªæŸ¥è¯¢...\n", flush=True)
                time.sleep(delay)
            else:
                print(f"[{keyword}] âœ… å®Œæˆï¼ˆæœ€åä¸€ä¸ªå…³é”®è¯ï¼‰\n", flush=True)

        except Exception as e:
            print(f"[{keyword}] âŒ è·å–æ•°æ®å¤±è´¥: {str(e)}", flush=True)
            results[keyword] = None

            # å¦‚æœé‡åˆ°é”™è¯¯ï¼Œå¢åŠ é¢å¤–ç­‰å¾…æ—¶é—´
            print(f"[{keyword}] ç­‰å¾… 10 ç§’åç»§ç»­...", flush=True)
            time.sleep(10)

    print(f"\n{'='*60}", flush=True)
    print(f"æ‰¹é‡æŸ¥è¯¢å®Œæˆï¼æˆåŠŸ: {len([k for k,v in results.items() if v])}/{len(keywords)}", flush=True)
    print(f"{'='*60}\n", flush=True)

    return results

def save_related_queries(keyword, related_data):
    """
    ä¿å­˜ç›¸å…³æŸ¥è¯¢æ•°æ®åˆ°JSONæ–‡ä»¶
    """
    if not related_data:
        return
    
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    json_data = {
        'keyword': keyword,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'related_queries': {
            'top': related_data['top'].to_dict(orient='records') if isinstance(related_data.get('top'), pd.DataFrame) else related_data.get('top'),
            'rising': related_data['rising'].to_dict(orient='records') if isinstance(related_data.get('rising'), pd.DataFrame) else related_data.get('rising')
        }
    }
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    filename = f"related_queries_{keyword}_{timestamp}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    
    return filename

def print_related_queries(related_data):
    """
    æ‰“å°ç›¸å…³æŸ¥è¯¢è¯æ•°æ®
    """
    if not related_data:
        print("æ²¡æœ‰ç›¸å…³æŸ¥è¯¢æ•°æ®")
        return
    
    print("\nç›¸å…³æŸ¥è¯¢è¯ç»Ÿè®¡:")
    print("=" * 50)
    
    # æ‰“å°çƒ­é—¨æŸ¥è¯¢
    if 'top' in related_data and related_data['top'] is not None:
        print("\nçƒ­é—¨æŸ¥è¯¢:")
        print("-" * 30)
        df = related_data['top']
        if isinstance(df, pd.DataFrame):
            for _, row in df.iterrows():
                print(f"- {row['query']:<30} (ç›¸å…³åº¦: {row['value']})")
    
    # æ‰“å°ä¸Šå‡è¶‹åŠ¿æŸ¥è¯¢
    if 'rising' in related_data and related_data['rising'] is not None:
        print("\nä¸Šå‡è¶‹åŠ¿æŸ¥è¯¢:")
        print("-" * 30)
        df = related_data['rising']
        if isinstance(df, pd.DataFrame):
            for _, row in df.iterrows():
                print(f"- {row['query']:<30} (å¢é•¿: {row['value']})")


# ä¸»å‡½æ•°
# timeframeå¯èƒ½çš„å€¼ï¼š
# today 12-mï¼š12ä¸ªæœˆ
# now 1-dï¼š1å¤©
# now 7-dï¼š7å¤©
# now 30-dï¼š30å¤©
# now 90-dï¼š90å¤©
# æ—¥æœŸæ ¼å¼ï¼š2024-12-28 2024-12-30
def main():
    # è®¾ç½®è¦æŸ¥è¯¢çš„å…³é”®è¯åˆ—è¡¨
    keywords = ['game']  # å¯ä»¥æ·»åŠ å¤šä¸ªå…³é”®è¯
    geo = ''
    timeframe = 'now 1-d'
    
    print("å¼€å§‹æ‰¹é‡æŸ¥è¯¢...")
    print(f"åœ°åŒº: {geo if geo else 'å…¨çƒ'}")
    print(f"æ—¶é—´èŒƒå›´: {timeframe}")
    
    try:
        # æ‰¹é‡è·å–æ•°æ®
        results = batch_get_queries(
            keywords,
            geo=geo,
            timeframe=timeframe,
            delay_between_queries=100  # è®¾ç½®è¯·æ±‚é—´éš”
        )

        # å¤„ç†å’Œä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        for keyword, data in results.items():
            if data:
                print(f"\nå¤„ç† {keyword} çš„æ•°æ®:")
                print_related_queries(data)
                filename = save_related_queries(keyword, data)
                print(f"æ•°æ®å·²ä¿å­˜åˆ°æ–‡ä»¶: {filename}")
            else:
                print(f"\næœªèƒ½è·å– {keyword} çš„æ•°æ®")
                
    except Exception as e:
        print(f"æ‰¹é‡æŸ¥è¯¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")

class RequestLimiter:
    def __init__(self):
        self.requests = []  # å­˜å‚¨è¯·æ±‚æ—¶é—´æˆ³
        self.max_requests_per_min = 30  # æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°
        self.max_requests_per_hour = 200  # æ¯å°æ—¶æœ€å¤§è¯·æ±‚æ•°
        
    def can_make_request(self):
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å‘èµ·æ–°è¯·æ±‚"""
        current_time = time.time()
        
        # æ¸…ç†è¶…è¿‡1å°æ—¶çš„æ—§è¯·æ±‚è®°å½•
        self.requests = [t for t in self.requests if current_time - t < 3600]
        
        # è·å–æœ€è¿‘1åˆ†é’Ÿçš„è¯·æ±‚æ•°
        recent_min_requests = len([t for t in self.requests if current_time - t < 60])
        
        # è·å–æœ€è¿‘1å°æ—¶çš„è¯·æ±‚æ•°
        recent_hour_requests = len(self.requests)
        
        if (recent_min_requests >= self.max_requests_per_min or 
            recent_hour_requests >= self.max_requests_per_hour):
            return False
        
        return True
    
    def add_request(self):
        """è®°å½•æ–°çš„è¯·æ±‚"""
        self.requests.append(time.time())
    
    def wait_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…ç›´åˆ°å¯ä»¥å‘é€è¯·æ±‚"""
        while not self.can_make_request():
            wait_time = random.uniform(5, 10)
            print(f"è¾¾åˆ°è¯·æ±‚é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
            time.sleep(wait_time)
        self.add_request()

# åˆ›å»ºå…¨å±€è¯·æ±‚é™åˆ¶å™¨
request_limiter = RequestLimiter()

if __name__ == "__main__":
    main()
